{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "169dc2c1",
      "metadata": {
        "scrolled": false,
        "id": "169dc2c1"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import math\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ffc9c6e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ffc9c6e7",
        "outputId": "db19e4f8-cba8-4381-8783-e26c61bc961b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 1.1204 - accuracy: 0.3967\n",
            "Epoch 2/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 1.0557 - accuracy: 0.4400\n",
            "Epoch 3/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 1.0253 - accuracy: 0.4733\n",
            "Epoch 4/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 1.0046 - accuracy: 0.4888\n",
            "Epoch 5/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9913 - accuracy: 0.5162\n",
            "Epoch 6/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9814 - accuracy: 0.5257\n",
            "Epoch 7/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9761 - accuracy: 0.5279\n",
            "Epoch 8/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9714 - accuracy: 0.5377\n",
            "Epoch 9/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9640 - accuracy: 0.5445\n",
            "Epoch 10/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9602 - accuracy: 0.5487\n",
            "Epoch 11/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9570 - accuracy: 0.5485\n",
            "Epoch 12/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9536 - accuracy: 0.5525\n",
            "Epoch 13/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 0.9495 - accuracy: 0.5491\n",
            "Epoch 14/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 0.9460 - accuracy: 0.5525\n",
            "Epoch 15/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9428 - accuracy: 0.5527\n",
            "Epoch 16/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9385 - accuracy: 0.5577\n",
            "Epoch 17/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9344 - accuracy: 0.5553\n",
            "Epoch 18/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9328 - accuracy: 0.5612\n",
            "Epoch 19/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9297 - accuracy: 0.5635\n",
            "Epoch 20/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9281 - accuracy: 0.5622\n",
            "Epoch 21/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9247 - accuracy: 0.5663\n",
            "Epoch 22/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9246 - accuracy: 0.5653\n",
            "Epoch 23/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9213 - accuracy: 0.5641\n",
            "Epoch 24/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 0.9200 - accuracy: 0.5676\n",
            "Epoch 25/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 0.9188 - accuracy: 0.5656\n",
            "Epoch 26/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9150 - accuracy: 0.5714\n",
            "Epoch 27/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9167 - accuracy: 0.5701\n",
            "Epoch 28/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9139 - accuracy: 0.5718\n",
            "Epoch 29/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9129 - accuracy: 0.5680\n",
            "Epoch 30/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9110 - accuracy: 0.5729\n",
            "Epoch 31/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9117 - accuracy: 0.5730\n",
            "Epoch 32/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9089 - accuracy: 0.5721\n",
            "Epoch 33/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9081 - accuracy: 0.5707\n",
            "Epoch 34/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9085 - accuracy: 0.5701\n",
            "Epoch 35/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 0.9095 - accuracy: 0.5716\n",
            "Epoch 36/50\n",
            "272/272 [==============================] - 1s 4ms/step - loss: 0.9042 - accuracy: 0.5758\n",
            "Epoch 37/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.9060 - accuracy: 0.5683\n",
            "Epoch 38/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9036 - accuracy: 0.5716\n",
            "Epoch 39/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9044 - accuracy: 0.5737\n",
            "Epoch 40/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9026 - accuracy: 0.5729\n",
            "Epoch 41/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9030 - accuracy: 0.5746\n",
            "Epoch 42/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9011 - accuracy: 0.5701\n",
            "Epoch 43/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9019 - accuracy: 0.5708\n",
            "Epoch 44/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.9003 - accuracy: 0.5738\n",
            "Epoch 45/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.8995 - accuracy: 0.5741\n",
            "Epoch 46/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.8976 - accuracy: 0.5749\n",
            "Epoch 47/50\n",
            "272/272 [==============================] - 1s 5ms/step - loss: 0.9022 - accuracy: 0.5737\n",
            "Epoch 48/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.8990 - accuracy: 0.5700\n",
            "Epoch 49/50\n",
            "272/272 [==============================] - 1s 3ms/step - loss: 0.8986 - accuracy: 0.5717\n",
            "Epoch 50/50\n",
            "272/272 [==============================] - 1s 2ms/step - loss: 0.8980 - accuracy: 0.5744\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ecacf321ccf5>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# io1.loading_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-3ff152474585>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' Final accuracy : {model.evaluate(self.test_Data , self.test_Label)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# per = tree.DecisionTreeClassifier()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1948\u001b[0m             )\n\u001b[1;32m   1949\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1950\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 20\n  y sizes: 1586\nMake sure all arrays contain the same number of samples."
          ]
        }
      ],
      "source": [
        "   # io = Clustering.Clustring()\n",
        "   # io.main()\n",
        "io1 =  BOW()\n",
        "# io1.getDatas()\n",
        "\n",
        "#     p = Train_Test.TrainTest()\n",
        "#     p.main()\n",
        "# io1.loading_data()\n",
        "evaluation = TrainTest()\n",
        "evaluation.main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3e41f92c",
      "metadata": {
        "id": "3e41f92c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BOW:\n",
        "    numberOfTrain = np.empty((8696, 20))\n",
        "    numberOfTest = np.empty((1587, 20))\n",
        "    train_Cluster = []\n",
        "    # we all of the comments of train and test files in these lists\n",
        "    df1 = []\n",
        "    df1_test = []\n",
        "    # .........................we have al of te words in words list\n",
        "    words = []\n",
        "    numbers = []\n",
        "    wordest = [[] for _ in range(0, 8709)]\n",
        "    test_Wordest = [[] for _ in range(0, 1610)]\n",
        "    vectors = []\n",
        "    cluster_repitition = np.zeros((10))\n",
        "    def getDatas(self):\n",
        "        df = pd.read_csv(\"dataset/train.csv\")\n",
        "        self.comment_dataframe = df[\"Comment\"]\n",
        "        df2 = pd.read_csv(\"dataset/test.csv\")\n",
        "        self.comment_dataframe_test = df2[\"Comment\"]\n",
        "\n",
        "        self.make_Dic( self.comment_dataframe , self.comment_dataframe_test)\n",
        "\n",
        "\n",
        "\n",
        "    def calculateBOW(self ,wordset,l_doc):\n",
        "        tf_diz = dict.fromkeys(wordset,0)\n",
        "        for word in l_doc:\n",
        "\n",
        "            tf_diz[word]=l_doc.count(word)\n",
        "        return tf_diz\n",
        "\n",
        "#     if you run the function below you don not need to run it again\n",
        "    def make_Dic(self , train_comments , test_comments):\n",
        "        # in this function we extract the words which are exist in Word2Vec and the length of them is more than 3\n",
        "        wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "\n",
        "        wordest_train = []\n",
        "        for i in range(0 ,len(train_comments) ):\n",
        "            l_doc = re.sub(r\"[^a-zA-Z]\", \" \", train_comments[i].lower()).split()\n",
        "            wordest_train.append(l_doc)\n",
        "        with open('wordest.txt', 'w') as f:\n",
        "            for line in wordest_train:\n",
        "                for word in line:\n",
        "                    if(len(word) > 3 ) and wv.has_index_for(word) :\n",
        "                        f.write(word)\n",
        "                    f.write(\" \")\n",
        "\n",
        "                f.write('\\n')\n",
        "\n",
        "        wordest_test = []\n",
        "        for i in range(0 ,len(test_comments) ):\n",
        "            l_doc = re.sub(r\"[^a-zA-Z]\", \" \", test_comments[i].lower()).split()\n",
        "            wordest_test.append(l_doc)\n",
        "        with open('wordest_test.txt', 'w') as f:\n",
        "            for line in wordest_test:\n",
        "                for word in line:\n",
        "                    if(len(word) > 3 ) and wv.has_index_for(word):\n",
        "                        f.write(word)\n",
        "                    f.write(\" \")\n",
        "\n",
        "                f.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       in the loop below we delete those that are repeated more than once\n",
        "        words = np.union1d(wordest_train[0] , wordest_train[1])\n",
        "        for index in range(2 , len(train_comments)) :\n",
        "            words = np.union1d(words , wordest_train[index])\n",
        "        for index in range( 0 , len(test_comments)):\n",
        "            words = np.union1d(words , wordest_test[index])\n",
        "\n",
        "#          in the loop below, we eliminate the words which their lentgh is less than 4 due to the fact we guess these\n",
        "#          words are prepositions\n",
        "        lentgh = len(words)\n",
        "        t = 0\n",
        "        for index in range(0, lentgh):\n",
        "            if len(words[index - t]) <= 3:\n",
        "                words = np.delete(words, index - t)\n",
        "                t += 1\n",
        "\n",
        "\n",
        "#       in the loop below we delete the words which are not exist in Word2Vec list\n",
        "        length= len(words)\n",
        "        y = 0\n",
        "        for io in range( 0 , length ):\n",
        "            if not wv.has_index_for(words[io - y]):\n",
        "                words = np.delete(words , io - y)\n",
        "                y += 1\n",
        "#       finally we write all of the words in a txt file\n",
        "        with open('readme.txt', 'w') as f:\n",
        "            for line in words:\n",
        "                f.write(line)\n",
        "                f.write('\\n')\n",
        "\n",
        "\n",
        "    def loading_data(self ) :\n",
        "        df = pd.read_csv(\"dataset/train.csv\")\n",
        "        train_comments = df[\"Comment\"]\n",
        "        df2 = pd.read_csv(\"dataset/test.csv\")\n",
        "        test_comments = df2[\"Comment\"]\n",
        "\n",
        "        with open('readme.txt', 'r') as file:\n",
        "\n",
        "            for line in file:\n",
        "                for word in line.split() :\n",
        "                    # displaying the words\n",
        "                    self.words.append(word)\n",
        "\n",
        "        with open('wordest.txt', 'r') as file:\n",
        "            index = 0\n",
        "            for line in file:\n",
        "                for word in line.split():\n",
        "                    # displaying the words\n",
        "                    self.wordest[index].append(word)\n",
        "                index +=1\n",
        "\n",
        "        with open('wordest_test.txt', 'r') as file:\n",
        "            index = 0\n",
        "            for line in file:\n",
        "                for word in line.split():\n",
        "                    # displaying the words\n",
        "                    self.test_Wordest[index].append(word)\n",
        "                index += 1\n",
        "\n",
        "        wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "\n",
        "#     in this loop we vectroize the words with W2V\n",
        "        for l in range(0 , len(self.words)) :\n",
        "            vector = wv.get_vector(self.words[l])\n",
        "            self.vectors.append(vector)\n",
        "\n",
        "        kmeans = KMeans(n_clusters=20, random_state=0, n_init=\"auto\").fit(self.vectors)\n",
        "\n",
        "\n",
        "#     in this loop we count the number of each word in any cluster\n",
        "\n",
        "        for index in range(0, len(train_comments)):\n",
        "            for word in self.wordest[index] :\n",
        "                self.numberOfTrain[index][kmeans.predict([wv.get_vector(word)])[0]] += 1\n",
        "                print(index)\n",
        "\n",
        "        for index in range(0 , len(test_comments)):\n",
        "            for word in self.test_Wordest[index]:\n",
        "                self.numberOfTrain[index][kmeans.predict([wv.get_vector(word)])[0]] += 1\n",
        "\n",
        "\n",
        "\n",
        "        with open('numbersOfTrain.txt' , 'w' ) as f:\n",
        "            for i in range(0 , len(self.numberOfTrain)):\n",
        "                for j in range(0 , len(self.numberOfTrain[i])):\n",
        "                    f.write(str(self.numberOfTrain[i][j]))\n",
        "                    f.write(\" \")\n",
        "                f.write('\\n')\n",
        "\n",
        "        with open('numbersOfTest.txt', 'w') as f:\n",
        "            for i in range(0, len(self.numberOfTest)):\n",
        "                for j in range(0, len(self.numberOfTest[i])):\n",
        "                    f.write(str(self.numberOfTest[i][j]))\n",
        "                    f.write(\" \")\n",
        "                f.write('\\n')\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "df131ed3",
      "metadata": {
        "id": "df131ed3"
      },
      "outputs": [],
      "source": [
        "class TrainTest:\n",
        "\n",
        "    train_Data = []\n",
        "    train_Label = []\n",
        "\n",
        "    test_Data=[]\n",
        "    test_Label = []\n",
        "\n",
        "    def main(self):\n",
        "\n",
        "        with open('numbersOfTrain.txt', 'r') as file:\n",
        "            for line in file:\n",
        "                vec = []\n",
        "                for word in line.split():\n",
        "                    vec.append(float(word))\n",
        "                self.train_Data.append(vec)\n",
        "        self.train_Data.pop()\n",
        "        with open('numbersOfTest.txt', 'r') as file:\n",
        "            for line in file:\n",
        "                vec = []\n",
        "                for word in line.split():\n",
        "                    vec.append(float(word))\n",
        "                self.test_Data.append(vec)\n",
        "        self.test_Data = self.test_Data.pop()\n",
        "        df = pd.read_csv(\"dataset/train.csv\")\n",
        "        df = df[\"Topic\"]\n",
        "        for i in range(0 , len(df)):\n",
        "            s = 0\n",
        "            if df[i] == \"Biology\" :\n",
        "                s = 0\n",
        "            elif df[i] == (\"Chemistry\"):\n",
        "                s = 1\n",
        "            else:\n",
        "                s = 2\n",
        "\n",
        "            self.train_Label.append(s)\n",
        "\n",
        "        df1 = pd.read_csv(\"dataset/test.csv\")\n",
        "        df1 = df1[\"Topic\"]\n",
        "        for i in range(0, len(df1)):\n",
        "            s = 0\n",
        "            if df[i]==(\"Biology\"):\n",
        "                s = 0\n",
        "            elif df[i]==(\"Chemistry\"):\n",
        "                s = 1\n",
        "            else:\n",
        "                s = 2\n",
        "\n",
        "            self.test_Label.append(s)\n",
        "\n",
        "\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(10, activation='relu', input_dim=20))\n",
        "        model.add(Dense(5, activation='relu'))\n",
        "        model.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        model.fit(self.train_Data, self.train_Label, epochs=50, batch_size=32)\n",
        "\n",
        "        print(f' Final accuracy : {model.evaluate(self.test_Data , self.test_Label)}')\n",
        "\n",
        "        # per = tree.DecisionTreeClassifier()\n",
        "        # per = per.fit(self.train_Data, self.train_Label)\n",
        "\n",
        "        # test = per.predict(self.test_Data)\n",
        "        # result = accuracy_score(test, self.test_Label)\n",
        "        # print(result)\n",
        "\n",
        "#         clf = Perceptron(tol=1e-4, random_state=1)\n",
        "#         clf.fit(self.train_Data, self.train_Label)\n",
        "#         print(clf.score(self.train_Data, self.train_Label))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425c4fb9",
      "metadata": {
        "id": "425c4fb9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b57bc4e3",
      "metadata": {
        "id": "b57bc4e3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06e55cd",
      "metadata": {
        "id": "a06e55cd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}