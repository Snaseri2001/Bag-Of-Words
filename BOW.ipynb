{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169dc2c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import math\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc9c6e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8696,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# io1.getDatas()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     p = Train_Test.TrainTest()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     p.main()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# io1.loading_data()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m TrainTest()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mTrainTest.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_Label\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[1;32m     51\u001b[0m per \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier()\n\u001b[0;32m---> 52\u001b[0m per \u001b[38;5;241m=\u001b[39m \u001b[43mper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_Data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_Label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m test \u001b[38;5;241m=\u001b[39m per\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_Data)\n\u001b[1;32m     55\u001b[0m result \u001b[38;5;241m=\u001b[39m accuracy_score(test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_Label)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:242\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    238\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    239\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 242\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:616\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[1;32m    615\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 616\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[1;32m    618\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8696,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "   # io = Clustering.Clustring()\n",
    "   # io.main()\n",
    "io1 =  BOW() \n",
    "# io1.getDatas()\n",
    "\n",
    "#     p = Train_Test.TrainTest()\n",
    "#     p.main()\n",
    "# io1.loading_data()\n",
    "evaluation = TrainTest()\n",
    "evaluation.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5403697",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=1000, random_state=0, n_init=\"auto\").fit(io1.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aa719cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/524.1 MB\u001b[0m \u001b[31m99.7 kB/s\u001b[0m eta \u001b[36m1:27:26\u001b[0m^C\n",
      "\u001b[2K     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/524.1 MB\u001b[0m \u001b[31m99.7 kB/s\u001b[0m eta \u001b[36m1:27:26\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e41f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BOW:\n",
    "    numberOfTrain = np.empty((8696, 50))\n",
    "    numberOfTest = np.empty((1587, 50))\n",
    "    train_Cluster = []\n",
    "    # we all of the comments of train and test files in these lists\n",
    "    df1 = []\n",
    "    df1_test = []\n",
    "    # .........................we have al of te words in words list\n",
    "    words = []\n",
    "    numbers = []\n",
    "    wordest = [[] for _ in range(0, 8709)]\n",
    "    test_Wordest = [[] for _ in range(0, 1610)]\n",
    "    vectors = []\n",
    "    cluster_repitition = np.zeros((10))\n",
    "    def getDatas(self):\n",
    "        df = pd.read_csv(\"dataset/train.csv\")\n",
    "        self.comment_dataframe = df[\"Comment\"]\n",
    "        df2 = pd.read_csv(\"dataset/test.csv\")\n",
    "        self.comment_dataframe_test = df2[\"Comment\"]\n",
    "   \n",
    "        self.make_Dic( self.comment_dataframe , self.comment_dataframe_test)\n",
    "\n",
    "\n",
    "\n",
    "    def calculateBOW(self ,wordset,l_doc):\n",
    "        tf_diz = dict.fromkeys(wordset,0)\n",
    "        for word in l_doc:\n",
    "\n",
    "            tf_diz[word]=l_doc.count(word)\n",
    "        return tf_diz\n",
    "\n",
    "#     if you run the function below you don not need to run it again \n",
    "    def make_Dic(self , train_comments , test_comments):\n",
    "        # in this function we extract the words which are exist in Word2Vec and the length of them is more than 3 \n",
    "        wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        \n",
    "        \n",
    "        wordest_train = []\n",
    "        for i in range(0 ,len(train_comments) ):\n",
    "            l_doc = re.sub(r\"[^a-zA-Z]\", \" \", train_comments[i].lower()).split()\n",
    "            wordest_train.append(l_doc)\n",
    "        with open('wordest.txt', 'w') as f:\n",
    "            for line in wordest_train:\n",
    "                for word in line:\n",
    "                    if(len(word) > 3 ) and wv.has_index_for(word) :\n",
    "                        f.write(word)\n",
    "                    f.write(\" \")\n",
    "        \n",
    "                f.write('\\n')\n",
    "        \n",
    "        wordest_test = []\n",
    "        for i in range(0 ,len(test_comments) ):\n",
    "            l_doc = re.sub(r\"[^a-zA-Z]\", \" \", test_comments[i].lower()).split()\n",
    "            wordest_test.append(l_doc)\n",
    "        with open('wordest_test.txt', 'w') as f:\n",
    "            for line in wordest_test:\n",
    "                for word in line:\n",
    "                    if(len(word) > 3 ) and wv.has_index_for(word):\n",
    "                        f.write(word)\n",
    "                    f.write(\" \")\n",
    "        \n",
    "                f.write('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "#       in the loop below we delete those that are repeated more than once\n",
    "        words = np.union1d(wordest_train[0] , wordest_train[1])\n",
    "        for index in range(2 , len(train_comments)) :\n",
    "            words = np.union1d(words , wordest_train[index])\n",
    "        for index in range( 0 , len(test_comments)):\n",
    "            words = np.union1d(words , wordest_test[index])\n",
    "\n",
    "#          in the loop below, we eliminate the words which their lentgh is less than 4 due to the fact we guess these \n",
    "#          words are prepositions\n",
    "        lentgh = len(words)\n",
    "        t = 0\n",
    "        for index in range(0, lentgh):\n",
    "            if len(words[index - t]) <= 3:\n",
    "                words = np.delete(words, index - t)\n",
    "                t += 1\n",
    "                \n",
    "                \n",
    "#       in the loop below we delete the words which are not exist in Word2Vec list             \n",
    "        length= len(words)\n",
    "        y = 0\n",
    "        for io in range( 0 , length ):\n",
    "            if not wv.has_index_for(words[io - y]):\n",
    "                words = np.delete(words , io - y)\n",
    "                y += 1\n",
    "#       finally we write all of the words in a txt file    \n",
    "        with open('readme.txt', 'w') as f:\n",
    "            for line in words:\n",
    "                f.write(line)\n",
    "                f.write('\\n')\n",
    "        \n",
    "\n",
    "    def loading_data(self ) :\n",
    "        df = pd.read_csv(\"dataset/train.csv\")\n",
    "        train_comments = df[\"Comment\"]\n",
    "        df2 = pd.read_csv(\"dataset/test.csv\")\n",
    "        test_comments = df2[\"Comment\"]\n",
    "        \n",
    "        with open('readme.txt', 'r') as file:\n",
    "\n",
    "            for line in file:\n",
    "                for word in line.split() :\n",
    "                    # displaying the words\n",
    "                    self.words.append(word)\n",
    "\n",
    "        with open('wordest.txt', 'r') as file:\n",
    "            index = 0\n",
    "            for line in file:\n",
    "                for word in line.split():\n",
    "                    # displaying the words\n",
    "                    self.wordest[index].append(word)\n",
    "                index +=1\n",
    "\n",
    "        with open('wordest_test.txt', 'r') as file:\n",
    "            index = 0\n",
    "            for line in file:\n",
    "                for word in line.split():\n",
    "                    # displaying the words\n",
    "                    self.test_Wordest[index].append(word)\n",
    "                index += 1\n",
    "\n",
    "        wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "\n",
    "#     in this loop we vectroize the words with W2V \n",
    "        for l in range(0 , len(self.words)) :\n",
    "            vector = wv.get_vector(self.words[l])\n",
    "            self.vectors.append(vector)\n",
    "            \n",
    "        kmeans = KMeans(n_clusters=50, random_state=0, n_init=\"auto\").fit(self.vectors)\n",
    "        \n",
    "        \n",
    "#     in this loop we count the number of each word in any cluster\n",
    "\n",
    "        for index in range(0, len(train_comments)):\n",
    "            for word in self.wordest[index] :\n",
    "                self.numberOfTrain[index][kmeans.predict([wv.get_vector(word)])[0]] += 1\n",
    "                print(index)\n",
    "                \n",
    "        for index in range(0 , len(test_comments)):\n",
    "            for word in self.test_Wordest[index]:\n",
    "                self.numberOfTrain[index][kmeans.predict([wv.get_vector(word)])[0]] += 1\n",
    "                \n",
    "                \n",
    "    \n",
    "        with open('numbersOfTrain.txt' , 'w' ) as f:\n",
    "            for i in range(0 , len(self.numberOfTrain)):\n",
    "                for j in range(0 , len(self.numberOfTrain[i])):\n",
    "                    f.write(str(self.numberOfTrain[i][j]))\n",
    "                    f.write(\" \")\n",
    "                f.write('\\n')\n",
    "    \n",
    "        with open('numbersOfTest.txt', 'w') as f:\n",
    "            for i in range(0, len(self.numberOfTest)):\n",
    "                for j in range(0, len(self.numberOfTest[i])):\n",
    "                    f.write(str(self.numberOfTest[i][j]))\n",
    "                    f.write(\" \")\n",
    "                f.write('\\n')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df131ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest:\n",
    "\n",
    "    train_Data = []\n",
    "    train_Label = []\n",
    "\n",
    "    test_Data=[]\n",
    "    test_Label = []\n",
    "\n",
    "    def main(self):\n",
    "\n",
    "        with open('numbersOfTrain.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                vec = []\n",
    "                for word in line.split():\n",
    "                    vec.append(float(word))\n",
    "                self.train_Data.append(vec)\n",
    "\n",
    "        with open('numbersOfTest.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                vec = []\n",
    "                for word in line.split():\n",
    "                    vec.append(float(word))\n",
    "                self.test_Data.append(vec)\n",
    "        self.train_Data = sel\n",
    "        df = pd.read_csv(\"dataset/train.csv\")\n",
    "        df = df[\"Topic\"]\n",
    "        for i in range(0 , len(df)):\n",
    "            s = 0\n",
    "            if df[i] == \"Biology\" :\n",
    "                s = 0\n",
    "            elif df[i] == (\"Chemistry\"):\n",
    "                s = 1\n",
    "            else:\n",
    "                s = 2\n",
    "\n",
    "            self.train_Label.append(s)\n",
    "\n",
    "        df1 = pd.read_csv(\"dataset/test.csv\")\n",
    "        df1 = df1[\"Topic\"]\n",
    "        for i in range(0, len(df1)):\n",
    "            s = 0\n",
    "            if df[i]==(\"Biology\"):\n",
    "                s = 0\n",
    "            elif df[i]==(\"Chemistry\"):\n",
    "                s = 1\n",
    "            else:\n",
    "                s = 2\n",
    "\n",
    "            self.test_Label.append(s)\n",
    "\n",
    "        per = tree.DecisionTreeClassifier()\n",
    "        per = per.fit(self.train_Data, self.train_Label)\n",
    "        \n",
    "        test = per.predict(self.test_Data)\n",
    "        result = accuracy_score(test, self.test_Label)\n",
    "        print(result)\n",
    "\n",
    "#         clf = Perceptron(tol=1e-4, random_state=1)\n",
    "#         clf.fit(self.train_Data, self.train_Label)\n",
    "#         print(clf.score(self.train_Data, self.train_Label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c4fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bc4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e55cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
