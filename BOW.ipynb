{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "169dc2c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import math\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffc9c6e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# io = Clustering.Clustring()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# io.main()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m io1 \u001b[38;5;241m=\u001b[39m BOW()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mio1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDatas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     p = Train_Test.TrainTest()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     p.main()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mBOW.getDatas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment_dataframe_test \u001b[38;5;241m=\u001b[39m df2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_Dic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment_dataframe\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment_dataframe_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m, in \u001b[0;36mBOW.make_Dic\u001b[0;34m(self, train_comments, test_comments)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_Dic\u001b[39m(\u001b[38;5;28mself\u001b[39m , train_comments , test_comments):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# in this function we extract the words which are exist in Word2Vec and the length of them is more than 3 \u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     wv \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGoogleNews-vectors-negative300.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     wordest_train \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m ,\u001b[38;5;28mlen\u001b[39m(train_comments) ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "   # io = Clustering.Clustring()\n",
    "   # io.main()\n",
    "io1 = BOW()\n",
    "io1.getDatas()\n",
    "#     p = Train_Test.TrainTest()\n",
    "#     p.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e41f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BOW:\n",
    "    numberOfTrain = np.empty((10, 10))\n",
    "    numberOfTest = np.empty((10, 10))\n",
    "    train_Cluster = []\n",
    "    # we all of the comments of train and test files in these lists\n",
    "    df1 = []\n",
    "    df1_test = []\n",
    "    # .........................we have al of te words in words list\n",
    "    words = []\n",
    "    numbers = []\n",
    "    wordest = [[] for _ in range(0, 8709)]\n",
    "    test_Wordest = [[] for _ in range(0, 1610)]\n",
    "    vectors = []\n",
    "    cluster_repitition = np.zeros((10))\n",
    "    def getDatas(self):\n",
    "        df = pd.read_csv(\"dataset/train.csv\")\n",
    "        self.comment_dataframe = df[\"Comment\"]\n",
    "        df2 = pd.read_csv(\"dataset/test.csv\")\n",
    "        self.comment_dataframe_test = df2[\"Comment\"]\n",
    "   \n",
    "        self.make_Dic( self.comment_dataframe , self.comment_dataframe_test)\n",
    "\n",
    "\n",
    "\n",
    "    def calculateBOW(self ,wordset,l_doc):\n",
    "        tf_diz = dict.fromkeys(wordset,0)\n",
    "        for word in l_doc:\n",
    "\n",
    "            tf_diz[word]=l_doc.count(word)\n",
    "        return tf_diz\n",
    "\n",
    "#     if you run the function below you don not need to run it again \n",
    "    def make_Dic(self , train_comments , test_comments):\n",
    "        # in this function we extract the words which are exist in Word2Vec and the length of them is more than 3 \n",
    "        wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        \n",
    "        \n",
    "        wordest_train = []\n",
    "        for i in range(0 ,len(train_comments) ):\n",
    "            l_doc = re.sub(r\"[^a-zA-Z]\", \" \", train_comments[i].lower()).split()\n",
    "            wordest_train.append(l_doc)\n",
    "        with open('wordest.txt', 'w') as f:\n",
    "            for line in wordest_train:\n",
    "                for word in line:\n",
    "                    if(len(word) > 3 ) and wv.has_index_for(word) :\n",
    "                        f.write(word)\n",
    "                    f.write(\" \")\n",
    "        \n",
    "                f.write('\\n')\n",
    "        \n",
    "        wordest_test = []\n",
    "        for i in range(0 ,len(test_comments) ):\n",
    "            l_doc = re.sub(r\"[^a-zA-Z]\", \" \", data_frame1[i].lower()).split()\n",
    "            wordest_test.append(l_doc)\n",
    "        with open('wordest_test.txt', 'w') as f:\n",
    "            for line in wordest_test:\n",
    "                for word in line:\n",
    "                    if(len(word) > 3 ) and wv.has_index_for(word):\n",
    "                        f.write(word)\n",
    "                    f.write(\" \")\n",
    "        \n",
    "                f.write('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "#       in the loop below we delete those that are repeated more than once\n",
    "        words = np.union1d(wordest[0] , wordest[1])\n",
    "        for index in range(2 , len(train_comments)) :\n",
    "            words = np.union1d(words , wordest[index])\n",
    "        for index in range( 0 , len(test_comments)):\n",
    "            words = np.union1d(words , wordest1[index])\n",
    "\n",
    "#          in the loop below, we eliminate the words which their lentgh is less than 4 due to the fact we guess these \n",
    "#          words are prepositions\n",
    "        lentgh = len(words)\n",
    "        t = 0\n",
    "        for index in range(0, lentgh):\n",
    "            if len(words[index - t]) <= 3:\n",
    "                words = np.delete(words, index - t)\n",
    "                t += 1\n",
    "                \n",
    "                \n",
    "#       in the loop below we delete the words which are not exist in Word2Vec list             \n",
    "        length= len(words)\n",
    "        y = 0\n",
    "        for io in range( 0 , length ):\n",
    "            if not wv.has_index_for(words[io - y]):\n",
    "                words = np.delete(words , io - y)\n",
    "                y += 1\n",
    "#       finally we write all of the words in a txt file    \n",
    "        with open('readme.txt', 'w') as f:\n",
    "            for line in words:\n",
    "                f.write(line)\n",
    "                f.write('\\n')\n",
    "        \n",
    "\n",
    "    def Clustring(self , train_comments , test_comments) :\n",
    "        \n",
    "        with open('readme.txt', 'r') as file:\n",
    "\n",
    "            for line in file:\n",
    "                for word in line.split() :\n",
    "                    # displaying the words\n",
    "                    self.words.append(word)\n",
    "\n",
    "        with open('wordest.txt', 'r') as file:\n",
    "            index = 0\n",
    "            for line in file:\n",
    "                for word in line.split():\n",
    "                    # displaying the words\n",
    "                    self.wordest[index].append(word)\n",
    "                index +=1\n",
    "\n",
    "        with open('wordest_test.txt', 'r') as file:\n",
    "            index = 0\n",
    "            for line in file:\n",
    "                for word in line.split():\n",
    "                    # displaying the words\n",
    "                    self.test_Wordest[index].append(word)\n",
    "                index += 1\n",
    "\n",
    "        wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "\n",
    "#     in this loop we vectroize the words with W2V \n",
    "        for l in range(0 , len(self.words)) :\n",
    "            vector = wv.get_vector(self.words[l])\n",
    "            self.vectors.append(vector)\n",
    "            \n",
    "        kmeans = KMeans(n_clusters=1000, random_state=0, n_init=\"auto\").fit(vector)\n",
    "        \n",
    "        \n",
    "#     in this loop we count the number of each word in any cluster\n",
    "        for index in range(0 , len(self.words)) :\n",
    "            for p in range( 0 , len(train_comments)):\n",
    "                num = self.wordest[p].count(self.words[index])\n",
    "                self.numberOfTrain[p][kmeans.predict(wv.get_vector(self.words[index]))[0]] += num\n",
    "            for pk in range(0, len(test_comments)):\n",
    "                num = self.test_Wordest[pk].count(self.words[index])\n",
    "                self.numberOfTest[pk][kmeans.predict(wv.get_vector(self.words[index]))[0]] += num\n",
    "    \n",
    "        with open('numbersOfTrain.txt' , 'w' ) as f:\n",
    "            for i in range(0 , len(self.numberOfTrain)):\n",
    "                for j in range(0 , len(self.numberOfTrain[i])):\n",
    "                    f.write(str(self.numberOfTrain[i][j]))\n",
    "                    f.write(\" \")\n",
    "                f.write('\\n')\n",
    "    \n",
    "        with open('numbersOfTest.txt', 'w') as f:\n",
    "            for i in range(0, len(self.numberOfTest)):\n",
    "                for j in range(0, len(self.numberOfTest[i])):\n",
    "                    f.write(str(self.numberOfTest[i][j]))\n",
    "                    f.write(\" \")\n",
    "                f.write('\\n')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df131ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest:\n",
    "\n",
    "    train_Data = []\n",
    "    train_Label = []\n",
    "\n",
    "    test_Data=[]\n",
    "    test_Label = []\n",
    "\n",
    "    def main(self):\n",
    "\n",
    "        with open('Train_Representation.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                vec = []\n",
    "                for word in line.split():\n",
    "                    vec.append(float(word))\n",
    "                self.train_Data.append(vec)\n",
    "\n",
    "        with open('Test_Representation.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                vec = []\n",
    "                for word in line.split():\n",
    "                    vec.append(float(word))\n",
    "                self.test_Data.append(vec)\n",
    "\n",
    "        df = pd.read_csv(\"dataset/train.csv\")\n",
    "        df = df[\"Topic\"]\n",
    "        for i in range(0 , len(df)):\n",
    "            s = 0\n",
    "            if df[i] == \"Biology\" :\n",
    "                s = 0\n",
    "            elif df[i] == (\"Chemistry\"):\n",
    "                s = 1\n",
    "            else:\n",
    "                s = 2\n",
    "\n",
    "            self.train_Label.append(s)\n",
    "\n",
    "        df1 = pd.read_csv(\"dataset/test.csv\")\n",
    "        df1 = df1[\"Topic\"]\n",
    "        for i in range(0, len(df1)):\n",
    "            s = 0\n",
    "            if df[i]==(\"Biology\"):\n",
    "                s = 0\n",
    "            elif df[i]==(\"Chemistry\"):\n",
    "                s = 1\n",
    "            else:\n",
    "                s = 2\n",
    "\n",
    "            self.test_Label.append(s)\n",
    "\n",
    "        # per = tree.DecisionTreeClassifier()\n",
    "        # per = per.fit(self.train_Data, self.train_Label)\n",
    "        #\n",
    "        # test = per.predict(self.test_Data)\n",
    "        # result = accuracy_score(test, self.test_Label)\n",
    "        # print(result)\n",
    "\n",
    "        clf = Perceptron(tol=1e-4, random_state=1)\n",
    "        clf.fit(self.train_Data, self.train_Label)\n",
    "        print(clf.score(self.train_Data, self.train_Label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c4fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bc4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e55cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
